# Review of Reinforcement Learning
## 概述
强化学习：让智能体在**动态环境**中**学习动作**和**达成目标**。

智能：在**动态、开放环境**中**达到目标**的能力

智能体：有**目标**，**感知-动力**闭环的系统

学习：利用经验E在任务T上获得性能P的改善，则关于T和P，对E进行了学习

与监督学习的区别：监督学习使用带标注的训练集使智能体具备可泛化的决策能力。但不可能在所有情境下都获得既正确又有代表性的动作示例，智能体必须从自身经验中学习。

与无监督学习的区别：
无监督学习寻找未标注数据的隐含结构，强化学习最大化收益。

强化学习的问题：智能体如何高效地从与环境交互中学习，以获得自适应性？
特点：
1. 交互、试错学习
2. 无监督信号，只有奖励信号
3. 奖励不一定实时

从交互信息中发现因果关系、动作的后果，以及达到目标的方式。

智能体：进行学习及实施决策的机器叫作智能体

环境：智能体之外所有与其相互作用的事物叫作环境

环境的模型：智能体可以用来预测环境对其动作的反应的任何事物

## 赌博机
探索/利用（开发）
### 问题描述
![Alt text](image.png)
找到平均获益最大的臂
### 动作-价值方法
![Alt text](image-1.png)
*q~*~(a)* 是实际动作价值，*Q~t~(a)* 是t时刻对动作价值的估计。
#### 采样平均法
用动作的平均收益估计期望收益
### 平衡探索与利用：ε-贪心
贪心策略一直在守成（利用），不断选择获益最高的动作
ε-贪心：大概率贪心，以小概率ε随机选择动作
![Alt text](image-2.png)
#### 采样平均法的增量形式
平均动作-价值估计：Q = (Q × (N-1) + R) / N，把Q单独拿出来
Q = Q + (R - Q) / N
Q += (R - Q) / N
![Alt text](image-3.png)
#### 测试结果
![Alt text](image-4.png)
![Alt text](image-5.png)
### 非平稳问题
如果真实的动作价值随时间变化，采样平均法就不适用了（因为有时间的概念，近期的结果显然对估计更有帮助，久远的结果就不太有参考价值了）
**<u>引入指数因子</u>**
Q~n+1~ = Q~n~ + α(R~n~ - Q~n~)
对比之前的Q = Q + (R - Q) / N

Q~n+1~ = Q~n~ + α(R~n~ - Q~n~)
= (1-α)Q~n~ + αR~n~
即每次迭代时为之前的Q施加一个折扣权值(1-α)，并以一定的权重增加获益R，之前的估计获益就会随时间逐渐衰减。
![Alt text](image-6.png)
#### 收敛性
![Alt text](image-7.png)
### 乐观初始值
在进行上述问题求解时需要设置初始值Q~1~，之前设置的是0，但可以设置一个乐观初始值，如5，可以提供一种简单的试探方法。
![Alt text](image-8.png)
### 基于置信度上界UCB的动作选择
也是一种平衡探索-利用的策略，考虑动作估计值有多接近最大值和他们的不确定性（不确定性越高越需要尝试一下这个动作）
UCB估计其实就是在某个置信度下取置信区间的上界作为动作价值估计。
![Alt text](image-9.png)
$\sqrt[]{{logt}\over{N_t(a)}}$是不确定性度量，值越大说明在较长的时间内对a的探索较少，不确定性就大。
选一个价值估计大且不确定的动作
效果比$\epsilon$-贪心要好点
### 梯度赌博机算法
![Alt text](image-10.png)
第t次迭代：
以一定概率选中动作a，概率通过$H_t(a)$的softmax计算得到；
$H_{t+1}(a)$的更新公式如图，$1\{A_t=a\}$是指示函数，如果第t次迭代选择的动作是a则函数值为1，反之为0；每次迭代都要对所有动作的$H$进行更新，更新公式由梯度下降推得；
平均收益的计算如图。
### 赌博机算法比较
![Alt text](image-11.png)

## 问题描述
非关联任务
关联任务

## DP
### 策略迭代
策略评估+策略改进
![Alt text](image-32.png)
### 价值迭代
在评估时更新策略
![Alt text](image-33.png)
## DP与MC的对比
![Alt text](image-14.png)
![Alt text](image-15.png)
![Alt text](image-16.png)
### DP
![Alt text](image-17.png)
动态规划的策略评估，然后贪心进行策略优化，这个是标准的DP
![Alt text](image-18.png)
![Alt text](image-19.png)
价值迭代，直接取动作的最大值作为动作选择（合并了策略评估和策略改进）

异步DP：前面都是同步DP，即价值的更新是同步的，需要先用一个数组v存储V，再对V的所有状态 ***同时*** 进行更新。现采用异步DP，就是每个状态直接就地更新，不需要同时对整个V进行更新
### MC
#### 策略评估
对各状态价值进行评估
![Alt text](image-20.png)
#### 策略控制
还要对策略进行优化改进，所以需要计算状态-动作价值即Q
![Alt text](image-21.png)
存在问题：不能保证对每个动作都探索到（因为一直贪心选择，只有利用没有探索）
需要软策略
#### 引入软策略
##### 同轨
待学习策略$\pi$本身就是一个软策略，同轨是指行动策略和需要学习的策略一致，怎么学就怎么动。
这里以$\epsilon$-贪心策略为软策略。
![Alt text](image-22.png)
##### 离轨
从别人的行动经验中学习，待学习策略和行动（采样）策略不一致。可以从一个软策略中学习，最终得到一个贪心策略。
离轨评估
![Alt text](image-23.png)
离轨控制
使用贪心策略为待学习策略$\pi$，因为是贪心策略，所以W更新时分子就直接是1了（只会采用最优的动作）
![Alt text](image-24.png)
## TD(0)
### Sarsa
![Alt text](image-13.png)
### Q-Learning
![Alt text](image-12.png)
### 对比
在状态-价值更新时不同；并且Sarsa的下一个A承接A'，因为是同轨的，待评估策略与行动策略一致。但Q-Learning就不是了，A是待学习策略决定的，而后继动作A'是贪心策略选择的，所以不能直接挪用。
## TD(n)
以n步采样得到的总收益G作为预计收益
### 策略评估
![Alt text](image-25.png)
### 策略控制
#### n步SARSA
进行n步采样，用n步采样的总收益G代替SARSA中一步采样的收益R
![Alt text](image-26.png)
相对于单步SARSA加快了策略学习
#### 离轨
##### 利用重要性采样
![Alt text](image-27.png)
##### n步回溯树：不用重要性采样
直接用回溯树算出每步期望获益，就不需要再从b中去采样$\pi$了
![Alt text](image-30.png)
![Alt text](image-29.png)
![Alt text](image-28.png)

## 策略梯度方法
动作价值方法：基于动作价值函数选择策略

策略梯度方法：把策略用参数进行表示，$\pi(s, \theta)$ ，策略改进就是对$\theta$做关于性能 $J(\theta)$ 的梯度上升。$\theta_{t+1}=\theta_t+\alpha\nabla{J(\theta)}$ 。直接学习策略（动作选择）不依赖于动作价值估计。带有动作-状态价值的策略梯度方法成为“行动器-评判器”

相较于动作价值的好处：以随机概率选择动作（softmax），可以学到一个随机的策略而不是确定的动作选择。好处是可以接近于一个确定的策略，同时以一个任意的概率来选择动作（因为有些时候最佳策略可能是概率的，如扑克游戏）。另一个优势是动作概率平滑变化，基于动作价值估计的方法可能因为动作价值函数的微小变化导致动作选择概率的突变（ $\epsilon$ - 贪心），但策略梯度方法参数是连续变化的，策略也是平滑变化。

### 策略梯度定理
![Alt text](image-31.png)
其中 $\mu$ 是分布，用这个定理得到 $J(\theta)$ 正比于某个式子，系数可以被梯度上升法的 $\alpha$ 包括。因此可以用后边那个式子作为梯度上升法的梯度。

### REINFORCE
REINFORCE算法就是基于策略梯度定理的一种策略梯度算法，也是一种蒙特卡洛算法。